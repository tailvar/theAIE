{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f93b45-b94f-4c64-beb2-0c7d7465cfd2",
   "metadata": {},
   "source": [
    "<b><u><span style=\"font-size:18px\">Week 2 Review Questions</span></u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b36a42-b43d-4c77-98d5-41149a72a863",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px\">Implement one manual gradient in numpy and verify it with a finite difference check</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b095cce6-0cb7-4520-8ecf-085190f52075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient:       [ 3.6  7.2 -3.6]\n",
      "Finite-diff gradient:  [ 3.6  7.2 -3.6]\n",
      "Close? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(w, x, t):\n",
    "    \"\"\"Scalar loss l(w) = 0.5 * (w.x - t)^2\"\"\"\n",
    "    y = np.dot(w,x)\n",
    "    return 0.5 * (y - t)**2\n",
    "\n",
    "def manual_grad(w, x, t):\n",
    "    \"\"\"Analytical gradient ∇_w L = (w.x - t) * x\"\"\"\n",
    "    y = np.dot(w, x)\n",
    "    return (y - t) * x\n",
    "\n",
    "def finite_diff_grad(w, x, t, eps=1e-6):\n",
    "    \"\"\"finite-difference approximation to ∇_w L.\"\"\"\n",
    "    grad_fd = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        w_pos = w.copy()\n",
    "        w_neg = w.copy()\n",
    "        w_pos[i] += eps\n",
    "        w_neg[i] -= eps\n",
    "        L_pos = loss(w_pos, x, t)\n",
    "        L_neg = loss(w_neg, x, t)\n",
    "        grad_fd[i] = (L_pos - L_neg) / (2.0 * eps) # central difference\n",
    "    return grad_fd\n",
    "\n",
    "# example values\n",
    "w = np.array([5.0, -0.3, 0.1])\n",
    "x = np.array([1.0, 2.0, -1.0])\n",
    "t = 0.7\n",
    "\n",
    "g_manual = manual_grad(w, x, t)\n",
    "g_fd = finite_diff_grad(w, x, t)\n",
    "\n",
    "print(\"Manual gradient:      \",g_manual)\n",
    "print(\"Finite-diff gradient: \",g_fd)\n",
    "print(\"Close?\", np.allclose(g_manual, g_fd, rtol=1e-5, atol=1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f384b53-074a-46d6-a779-d1f4c9624812",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px\">Build a minimal nn.module; run one epoch over a synthetic dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4b90f8f-d79e-4f67-b660-bf7115d3cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss after one epoch: 0.6693\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# synthetic dataset\n",
    "# X: points in R^2, y: binary label based on radius\n",
    "N = 512\n",
    "X = torch.randn(N,2)\n",
    "r2 = (X[:, 0]**2 + X[:, 1]**2)\n",
    "y = (r2 > 1.0).float().unsqueeze(1) # shape (N, 1), labels 0/1\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# minimal MLP as nn.module\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=16, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Linear(in_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) # logits\n",
    "\n",
    "model = TinyMLP()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "# one training epoch\n",
    "model.train() # training mode (important for dropout/BatchNorm; here just good habit)\n",
    "\n",
    "running_loss = 0.0\n",
    "for batch_idx, (xb, yb) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()         # clear old gradients\n",
    "\n",
    "    logits = model(xb)            # forward\n",
    "    loss = loss_fn(logits, yb)    # scalar loss\n",
    "\n",
    "    loss.backward()               # backward: compute grads\n",
    "    optimizer.step()              # update weights\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "avg_loss = running_loss / len(dataloader)\n",
    "print(f\"Average training loss after one epoch: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7789a96-6599-44ee-995c-ab223e62b9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.6507, train_acc=0.617 | val_loss=0.6598, val_acc=0.529 <-- best\n",
      "Epoch 02 | train_loss=0.6076, train_acc=0.620 | val_loss=0.6346, val_acc=0.529 <-- best\n",
      "Epoch 03 | train_loss=0.5603, train_acc=0.620 | val_loss=0.5706, val_acc=0.559 <-- best\n",
      "Epoch 04 | train_loss=0.5058, train_acc=0.688 | val_loss=0.5148, val_acc=0.657 <-- best\n",
      "Epoch 05 | train_loss=0.4466, train_acc=0.763 | val_loss=0.4528, val_acc=0.735 <-- best\n",
      "Epoch 06 | train_loss=0.3914, train_acc=0.878 | val_loss=0.3924, val_acc=0.863 <-- best\n",
      "Epoch 07 | train_loss=0.3359, train_acc=0.920 | val_loss=0.3417, val_acc=0.902 <-- best\n",
      "Epoch 08 | train_loss=0.2913, train_acc=0.959 | val_loss=0.2956, val_acc=0.961 <-- best\n",
      "Epoch 09 | train_loss=0.2573, train_acc=0.978 | val_loss=0.2633, val_acc=0.941 <-- best\n",
      "Epoch 10 | train_loss=0.2285, train_acc=0.978 | val_loss=0.2405, val_acc=0.931 <-- best\n",
      "Epoch 11 | train_loss=0.2067, train_acc=0.980 | val_loss=0.2176, val_acc=0.951 <-- best\n",
      "Epoch 12 | train_loss=0.1905, train_acc=0.983 | val_loss=0.2035, val_acc=0.941 <-- best\n",
      "Epoch 13 | train_loss=0.1748, train_acc=0.985 | val_loss=0.1906, val_acc=0.941 <-- best\n",
      "Epoch 14 | train_loss=0.1635, train_acc=0.980 | val_loss=0.1815, val_acc=0.931 <-- best\n",
      "Epoch 15 | train_loss=0.1530, train_acc=0.983 | val_loss=0.1671, val_acc=0.951 <-- best\n",
      "Epoch 16 | train_loss=0.1439, train_acc=0.985 | val_loss=0.1613, val_acc=0.951 <-- best\n",
      "Epoch 17 | train_loss=0.1363, train_acc=0.985 | val_loss=0.1508, val_acc=0.941 <-- best\n",
      "Epoch 18 | train_loss=0.1316, train_acc=0.976 | val_loss=0.1503, val_acc=0.941 <-- best\n",
      "Epoch 19 | train_loss=0.1240, train_acc=0.988 | val_loss=0.1464, val_acc=0.941 <-- best\n",
      "Epoch 20 | train_loss=0.1189, train_acc=0.983 | val_loss=0.1416, val_acc=0.941 <-- best\n",
      "\n",
      "Best validation loss: 0.1416\n",
      "Best checkpoint saved to: /home/oem/PycharmProjects/theAIE/aiecode/ch02/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# ----------------------------------------\n",
    "# Reproducibility - set seeds\n",
    "# ----------------------------------------\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Synthetic dataset: X: points in R^2, y: binary label based on radius\n",
    "# ----------------------------------------\n",
    "N = 512\n",
    "X = torch.randn(N,2)\n",
    "r2 = (X[:, 0]**2 + X[:, 1]**2)\n",
    "y = (r2 > 1.0).float().unsqueeze(1) # shape (N, 1), labels 0/1\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Train/validation split (80/20)\n",
    "val_fraction = 0.2\n",
    "n_val = int(math.floor(N * val_fraction))\n",
    "n_train = N - n_val\n",
    "\n",
    "# use a generator\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_ds, val_ds = random_split(dataset, [n_train, n_val], generator = g)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ----------------------------------------\n",
    "# minimal MLP model\n",
    "# ----------------------------------------\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=16, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = TinyMLP()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-2, weight_decay = 1e-4)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# training + validation loop with metrics and checking\n",
    "# ------------------------------------------------\n",
    "num_epochs = 20\n",
    "checkpoint_path = Path(\"best_model.pt\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ----------- TRAIN -------------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total =0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # train accuracy for this batch\n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            running_correct += (preds == yb).sum().item()\n",
    "            running_total += yb.size(0)\n",
    "\n",
    "    train_loss = running_loss / running_total\n",
    "    train_acc = running_correct / running_total\n",
    "\n",
    "    # ---------- VALIDATION ----------------------\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            val_loss_sum += loss.item() * xb.size(0)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            val_total += yb.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # --------- CHECKPOINTING -------------------\n",
    "    improved = val_loss < best_val_loss\n",
    "    if improved:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"seed\": SEED,\n",
    "            },\n",
    "            checkpoint_path,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f} \"\n",
    "        f\"{'<-- best' if improved else ''}\"\n",
    "        )\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best checkpoint saved to: {checkpoint_path.resolve()}\")\n",
    "        \n",
    "            \n",
    "            \n",
    "                    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2d5ce-f4e4-4d13-8915-0657ecf2bad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5cdc59-cd9b-4336-bf35-c9515ca3c99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d0ed2-011d-4b6b-b01a-cfc678812943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (theAIE)",
   "language": "python",
   "name": "theaie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
