{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dbb5b6-c441-4e2b-9b8b-dfdf72581116",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/tailvar/theAIE/blob/master/aiecode/ch04/Capstone/notebook/incident_mcp_colab_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518a98b-9a5b-48e9-8fac-6ef3d82c5335",
   "metadata": {},
   "source": [
    "# Incident MCP Capstone — Colab Demo (Optional)\n",
    "\n",
    "This notebook runs the **Incident Command Agent** end-to-end inside Colab.\n",
    "\n",
    "- **Canonical workflow remains local** (recommended).\n",
    "- This notebook is an **optional** demo path.\n",
    "\n",
    "It runs the agent, which **spawns the stdio MCP server as a subprocess** and communicates via newline-delimited JSON-RPC over pipes.\n",
    "\n",
    "This notebook does not change how the Incident Command Agent works. Colab is used purely as a lightweight execution environment. The \n",
    "agent and MCP server communicate using the same stdio-based JSON-RPC protocol as in local execution.\n",
    "\n",
    "**For simplicity and reproducibility, the deterministic rules planner is used by default. LLM-based planning is supported but requires\n",
    "external API keys and is therefore optional in Colab.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac58f7-9bc2-415f-88bb-a094b60a5314",
   "metadata": {},
   "source": [
    "## Why this notebook does “bootstrapping” only in Colab\n",
    "\n",
    "This notebook is meant to be a *thin demo* for Capstone 4 — it should run the project the same way it would run locally i.e. (`python -m incident_mcp ...`) without changing the real repository state.\n",
    "\n",
    "### Local Jupyter (running on \"local\" machine)\n",
    "When notebook runs locally, repository is already saved to disk. Therefore the expected directories already exist:\n",
    "\n",
    "- `config/` and `data/` are part of the repo\n",
    "- `artifacts/` is usually already present (or created by normal runs)\n",
    "- your file layout reflects the “real” project structure\n",
    "\n",
    "So **the notebook should not create or modify directories locally**, because doing so can hide genuine setup issues or create files not intended to be added to git.\n",
    "\n",
    "### Google Colab (ephemeral runtime)\n",
    "Colab starts from a clean Linux machine. Nothing exists until its created:\n",
    "\n",
    "- the repo must be cloned into the session\n",
    "- output directories like `artifacts/` won’t exist unless they are created\n",
    "- the memory log (`data/memory/memory.jsonl`) won’t exist until its created\n",
    "\n",
    "So in Colab theres a small amount of **bootstrapping** to make sure the demo can run end-to-end.\n",
    "\n",
    "### Key idea\n",
    "We only “bootstrap” in Colab to compensate for its ephemeral filesystem, while keeping local execution canonical and reproducible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6736f-a4ad-44a9-8979-6298dcf62485",
   "metadata": {},
   "source": [
    "## Where is notebook being run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e885ec-262b-412f-9ca6-706cf885867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB = False\n",
      "Notebook cwd = /home/oem/PycharmProjects/theAIE/aiecode/ch04/Capstone/notebook\n",
      "INCIDENT_MCP_ROOT = /home/oem/PycharmProjects/theAIE/aiecode/ch04/Capstone\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"pyproject.toml\").exists() and (parent / \"src\").exists():\n",
    "            return parent\n",
    "    raise RuntimeError(f\"Could not locate project root above {start}\")\n",
    "\n",
    "# Prefer robust detection; fallback to notebook heuristic\n",
    "try:\n",
    "    ROOT = find_project_root(cwd)\n",
    "except RuntimeError:\n",
    "    ROOT = cwd.parent if cwd.name == \"notebook\" else cwd\n",
    "\n",
    "os.environ[\"INCIDENT_MCP_ROOT\"] = str(ROOT)\n",
    "\n",
    "print(\"IN_COLAB =\", IN_COLAB)\n",
    "print(\"Notebook cwd =\", cwd)\n",
    "print(\"INCIDENT_MCP_ROOT =\", ROOT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e1884",
   "metadata": {},
   "source": [
    "## 1) Install + fetch the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f464c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Jupyter environment detected.\n"
     ]
    }
   ],
   "source": [
    "# This notebook is an OPTIONAL demo harness for Capstone 4.\n",
    "# It runs the Incident Command Agent exactly as it would run locally,\n",
    "# using stdio-based JSON-RPC between agent and server.\n",
    "\"\"\"\n",
    "OPTIONAL SETUP CELL\n",
    "\n",
    "• If running locally:\n",
    "    - Do NOT clone the repo\n",
    "    - Assume this notebook already lives inside the project tree\n",
    "\n",
    "• If running in Google Colab:\n",
    "    - Colab is a clean environment\n",
    "    - Optionally clone the repo after explicit confirmation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Google Colab environment detected.\")\n",
    "    answer = input(\n",
    "        \"You are running in Colab.\\n\"\n",
    "        \"This will clone the repository into the Colab runtime.\\n\"\n",
    "        \"Type 'yes' to proceed, anything else to skip: \"\n",
    "    ).strip().lower()\n",
    "\n",
    "    if answer == \"yes\":\n",
    "        # Minimal dependencies required for Capstone 4\n",
    "        !pip install -q python-dotenv\n",
    "\n",
    "        # Optional: only needed for LLM planning\n",
    "        !pip install -q anthropic openai\n",
    "\n",
    "        # Clone the repository (Colab only)\n",
    "        !git clone https://github.com/tailvar/theAIE.git\n",
    "   \n",
    "        # Move to Capstone directory\n",
    "        %cd theAIE/aiecode/ch04/Capstone\n",
    "\n",
    "        # Install the project in *editable* mode.\n",
    "        #\n",
    "        # This tells Python:\n",
    "        #   \"When importing `incident_mcp`, use the source code in ./src/\"\n",
    "        #\n",
    "        # Editable installs work very well in notebooks and development:\n",
    "        # - code changes picked up immediately\n",
    "        # - no files are copied\n",
    "        # - matches how project runs locally\n",
    "        !pip install -e .\n",
    "        \n",
    "        # Set protocol root for MCP\n",
    "        os.environ[\"INCIDENT_MCP_ROOT\"] = os.getcwd()\n",
    "        print(\"INCIDENT_MCP_ROOT =\", os.environ[\"INCIDENT_MCP_ROOT\"])\n",
    "    else:\n",
    "        raise RuntimeError(\"Repository clone skipped. Cannot proceed in Colab.\")\n",
    "\n",
    "else:\n",
    "    print(\"Local Jupyter environment detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3180bea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment detected.\n",
      "Assuming filesystem already exists; no directories created.\n"
     ]
    }
   ],
   "source": [
    "# --- Filesystem setup for Incident MCP Capstone ---\n",
    "# Local Jupyter: assume directories already exist (DO NOTHING)\n",
    "# Google Colab : create \"expected\" directories (ephemeral filesystem)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Colab detected: bootstrapping filesystem structure.\")\n",
    "\n",
    "    # Create expected directories (rules + LLM use the same layout)\n",
    "    (ROOT / \"artifacts\" / \"traces\").mkdir(parents=True, exist_ok=True)\n",
    "    (ROOT / \"artifacts\" / \"handoffs\").mkdir(parents=True, exist_ok=True)\n",
    "    (ROOT / \"data\" / \"memory\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure memory file exists (JSONL append-only)\n",
    "    mem_path = ROOT / \"data\" / \"memory\" / \"memory.jsonl\"\n",
    "    mem_path.touch(exist_ok=True)\n",
    "\n",
    "    print(\"Artifacts dir:\", ROOT / \"artifacts\")\n",
    "    print(\"Memory file  :\", mem_path)\n",
    "\n",
    "else:\n",
    "    print(\"Local environment detected.\")\n",
    "    print(\"Assuming filesystem already exists; no directories created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a5c9f-e54d-465f-9f79-3809117657a4",
   "metadata": {},
   "source": [
    "## 2) Choose planning mode\n",
    "\n",
    "### Option A — Rules mode (recommended for Colab)\n",
    "No API keys required, fully deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b536b7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLANNER_BACKEND= rules\n"
     ]
    }
   ],
   "source": [
    "# --- Rules mode (default, simplest) ---\n",
    "import os\n",
    "os.environ[\"PLANNER_BACKEND\"] = \"rules\"\n",
    "print(\"PLANNER_BACKEND=\", os.environ[\"PLANNER_BACKEND\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d481ddb-87cb-43b9-bc98-c8a8ebc8759b",
   "metadata": {},
   "source": [
    "### Option B — Enable LLM-based planning (Anthropic or OpenAI)\n",
    "Requires an API key. Use `getpass()` so you don't print secrets into outputs.\n",
    "By default, this notebook uses the deterministic RULES planner so it can run without external API keys.\n",
    "\n",
    "If you want to demonstrate LLM-based planning in Colab, uncomment ONE of the blocks below and provide your API key\n",
    "securely using getpass().\n",
    "\n",
    "NOTE:\n",
    " - Keys entered via getpass() are NOT stored in the notebook\n",
    " - They exist only for the lifetime of the Colab runtime\n",
    " - This mirrors best practice for ephemeral environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88901d3f-027c-42e7-84f1-e67ce827185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\n",
    "        \"Colab detected.\\n\"\n",
    "        \"If you want to run LLM mode, uncomment the optional cell below \"\n",
    "        \"to provide API keys using getpass().\\n\"\n",
    "        \"If running locally, skip this entirely.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a1c0a-2654-41cd-8b40-3665d0288d42",
   "metadata": {},
   "source": [
    "### Optional: LLM API Keys (Colab only)\n",
    "m\n",
    "If you are running this notebook locally, **do not run the cell below**.\n",
    "Local execution uses `.env` or shell environment variables.\n",
    "\n",
    "If you are running in Google Colab and want to demonstrate LLM-based\n",
    "planning, you may uncomment the cell below to enter your API key\n",
    "securely using `getpass()`. Keys entered this way are not stored and\n",
    "exist only for the lifetime of the Colab runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb867b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTIONAL: LLM mode (uncomment one block) ---\n",
    "\n",
    "# (1) Anthropic\n",
    "# from getpass import getpass\n",
    "# os.environ[\"PLANNER_BACKEND\"] = \"anthropic\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"Enter ANTHROPIC_API_KEY: \")\n",
    "# # Optional model override\n",
    "# # os.environ[\"ANTHROPIC_MODEL\"] = \"claude-3-5-sonnet-20241022\"\n",
    "\n",
    "# (2) OpenAI\n",
    "# from getpass import getpass\n",
    "# os.environ[\"PLANNER_BACKEND\"] = \"openai\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY: \")\n",
    "# # Optional model override\n",
    "# # os.environ[\"OPENAI_MODEL\"] = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ea4e0",
   "metadata": {},
   "source": [
    "## 3) Run the agent (agent spawns the server via stdio)\n",
    "\n",
    "This is the key point: **you do not start the server separately** in stdio mode.\n",
    "The agent starts it as a subprocess and talks JSON-RPC over stdin/stdout pipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8812fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m incident_mcp agent --root \"$INCIDENT_MCP_ROOT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe47c8",
   "metadata": {},
   "source": [
    "## 4) Inspect artifacts (telemetry, traces, memory, handoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3198cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cwd now: /home/oem/PycharmProjects/theAIE/aiecode/ch04/Capstone\n",
      "INCIDENT_MCP_ROOT: /home/oem/PycharmProjects/theAIE/aiecode/ch04/Capstone\n",
      "--- artifacts/ ---\n",
      "artifacts:\n",
      "handoffs\n",
      "runs.jsonl\n",
      "traces\n",
      "\n",
      "artifacts/handoffs:\n",
      "handoff_run-1d94794224.md\n",
      "handoff_run-48418aee93.md\n",
      "handoff_run-87c4418aa9.md\n",
      "handoff_run-9bf1ab6167.md\n",
      "\n",
      "artifacts/traces:\n",
      "run-029944aec7.jsonl\n",
      "run-15662265c8.jsonl\n",
      "run-179da361a4.jsonl\n",
      "run-1d94794224.jsonl\n",
      "run-24c28c0c5b.jsonl\n",
      "run-2af2d3e411.jsonl\n",
      "run-39e696da46.jsonl\n",
      "run-48418aee93.jsonl\n",
      "run-49c47816e1.jsonl\n",
      "run-87c4418aa9.jsonl\n",
      "run-8cc517464c.jsonl\n",
      "run-901b29ec07.jsonl\n",
      "run-996946ae60.jsonl\n",
      "run-9bf1ab6167.jsonl\n",
      "run-9ec55a73d7.jsonl\n",
      "run-c96c8d95d4.jsonl\n",
      "run-d3c19bc03d.jsonl\n",
      "run-e44e19b43c.jsonl\n",
      "run-eaa3c0860a.jsonl\n",
      "--------------------------------------------------------------\n",
      "\\n--- runs.jsonl (tail) ---\n",
      "{\"run_id\": \"run-1d94794224\", \"step\": 17, \"event\": \"finish\", \"ts_utc\": \"2026-01-05T07:10:54Z\", \"budgets\": {\"steps_used\": 17, \"tool_calls_used\": 3, \"elapsed_ms\": 637, \"tokens_used\": 0, \"cost_usd\": 0.0}, \"reason\": \"Handoff already drafted\"}\n",
      "--------------------------------------------------------------\n",
      "\\n--- memory.jsonl (tail) ---\n",
      "{\"alert_id\": \"A-1001\", \"kind\": \"tool_delta\", \"tool_name\": \"summarize_incident\", \"status\": \"success\", \"summary\": \"Drafted incident handoff note.\", \"metrics\": {\"latency_ms\": 0, \"cost_usd\": 0.0, \"tokens_in\": 0, \"tokens_out\": 0}, \"ts_utc\": \"2026-01-05T07:10:54Z\"}\n",
      "--------------------------------------------------------------\n",
      "\\n--- handoffs/ ---\n",
      "total 12\n",
      "-rw-rw-r-- 1 oem oem 1407 Jan  5 18:10 handoff_run-1d94794224.md\n",
      "-rw-rw-r-- 1 oem oem 2655 Jan  5 15:38 handoff_run-48418aee93.md\n",
      "-rw-rw-r-- 1 oem oem  708 Jan  4 18:32 handoff_run-87c4418aa9.md\n",
      "-rw-rw-r-- 1 oem oem    0 Dec 31 07:22 handoff_run-9bf1ab6167.md\n",
      "--------------------------------------------------------------\n",
      "\\n--- latest handoff (if present) ---\n",
      "# Incident Handoff\n",
      "\n",
      "**Alert ID:** A-1001\n",
      "\n",
      "## Evidence (high signal)\n",
      "- [retrieve_runbook] {\"query\": \"incident triage\", \"hits\": [{\"doc_id\": \"staging-api-crashloop.md\", \"chunk_id\": 0, \"score\": 1.0, \"text\": \"# Runbook: staging-api CrashLoopBackOff\\n\\n## Quick triage\\n1. List pods:\\n   - kubectl get pods -n staging\\n2. Inspect previous container logs:\\n   - kubectl logs <pod> -n staging --previous\\n3. Describe pod for events:\\n   - kubectl describe pod <pod> -n staging\\n\\n## Common causes\\n- bad env var / secret missing\\n- incompatible image tag\\n- migration running at boot and failing\\n- memory limit too low / OOMKill\\n\\n## Mitigations\\n- rollback last deploy\\n- increase memory limit temporarily\\n- disable failing feature flag if applicable\\n\", \"citation\": \"staging-api-crashloop.md#0\"}]}\n",
      "- [run_diagnostic] {\"host\": \"staging-api\", \"command\": \"kubectl get pods\", \"exit_code\": 0, \"stdout\": \"NAME                          READY   STATUS             RESTARTS   AGE\\nstaging-api-7c9d9f9f8f-abc    0/1     CrashLoopBackOff   7          12m\\nstaging-api-7c9d9f9f8f-def    0/1     CrashLoopBackOff   6          12m\\nstaging-api-7c9d9f9f8f-ghi    0/1     CrashLoopBackOff   8          12m\\n\", \"stderr\": \"\"}\n",
      "\n",
      "## Recommendation\n",
      "- Inspect logs of one failing pod (`kubectl logs <pod> --previous`).\n",
      "- Check recent deployment diff (image/env/secrets).\n",
      "- If cause identified, consider rollback/restart per runbook.\n"
     ]
    }
   ],
   "source": [
    "cwd = Path.cwd().resolve()\n",
    "\n",
    "# If you're inside Capstone/notebook, step up one level\n",
    "ROOT = cwd.parent if cwd.name == \"notebook\" else cwd\n",
    "\n",
    "# If you're not sure, you can harden this by looking for pyproject.toml\n",
    "if not (ROOT / \"pyproject.toml\").exists() and (cwd / \"pyproject.toml\").exists():\n",
    "    ROOT = cwd\n",
    "\n",
    "# Export so your code + subprocesses agree\n",
    "os.environ[\"INCIDENT_MCP_ROOT\"] = str(ROOT)\n",
    "\n",
    "# In notebooks, *also* change directory for shell commands like !ls\n",
    "os.chdir(ROOT)\n",
    "\n",
    "print(\"Notebook cwd now:\", Path.cwd())\n",
    "print(\"INCIDENT_MCP_ROOT:\", os.environ[\"INCIDENT_MCP_ROOT\"])\n",
    "\n",
    "!echo \"--- artifacts/ ---\"; ls -R artifacts | head -n 200\n",
    "print(\"--------------------------------------------------------------\")\n",
    "!echo \"\\n--- runs.jsonl (tail) ---\"; tail -n 1 artifacts/runs.jsonl\n",
    "print(\"--------------------------------------------------------------\")\n",
    "!echo \"\\n--- memory.jsonl (tail) ---\"; tail -n 1 data/memory/memory.jsonl\n",
    "print(\"--------------------------------------------------------------\")\n",
    "!echo \"\\n--- handoffs/ ---\"; ls -l artifacts/handoffs || true\n",
    "print(\"--------------------------------------------------------------\")\n",
    "!echo \"\\n--- latest handoff (if present) ---\"; (ls -t artifacts/handoffs/*.md 2>/dev/null | head -n 1 | xargs -r sed -n '1,200p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78980845-446a-47f9-8993-ff50d153d08c",
   "metadata": {},
   "source": [
    "## Notes / limitations\n",
    "\n",
    "- Colab runtimes are **ephemeral**: artifacts disappear when the runtime resets.\n",
    "- For persistence, mount Google Drive and point `artifacts/` and `data/` there.\n",
    "- For a *networked* demo (local agent connecting to Colab server), use the **WebSocket server** and a tunnel (advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003ad92-b5d5-4f65-b6c4-dcdd4f8bd075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
