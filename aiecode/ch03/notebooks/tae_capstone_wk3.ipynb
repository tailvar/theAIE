{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7886bf-2d75-4094-aea7-604b255523ce",
   "metadata": {},
   "source": [
    "# Tiny tranfformer LM (GPT-style)\n",
    "\n",
    "This notebook implements a tint decoder-only Transformer language model with:\n",
    "\n",
    "1. Configuration & Reproducible Seeding\n",
    "2. Data Utilities (character-level corpus, vocabulary, batching)\n",
    "3. Core modules:\n",
    "    - scaled dot product attention\n",
    "    - multi-head self attention\n",
    "    - feedforward block\n",
    "    - transformer block\n",
    "4. \"TinyTransformerLM\" model class\n",
    "5. Training loop with cross-entropy loss\n",
    "6. Logging, simple run-tracking and checkpoints\n",
    "7. Sampling / text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b799564-76ef-486b-bb3a-854d12fe242b",
   "metadata": {},
   "source": [
    "# The Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3c203-4496-407e-aa1b-c1c76799f9f6",
   "metadata": {},
   "source": [
    "1. 'TinyTransformerLM' embeds tokens amd positions -> produces initial representations.\n",
    "2. It sends then throgh a stack of *TransformerBlocks*\n",
    "    Each block consists of:\n",
    "        - *MultiHeadSelfAttention*: tokens communicate with past tokens and gather context\n",
    "        - *FeedForward*: transforms each tokens representation non-linearly\n",
    "        - Residual + LayerNorm help stability and flow\n",
    "3. The final hidden stattes go through a linear projection -> logits\n",
    "4. Logits give a probability distribution over the *next token* at each position.\n",
    "5. Training teaches the model to reduce negative log-liklihood (cross entropy).\n",
    "6. At generation time, the model samples tokens autoregressively using teh same logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47350b29-0d84-4e56-b2f6-e67390114b8d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f775f8ba-3bd6-4cf7-bfbe-795b85cf2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4de220-c66a-4460-8579-8f82caed9e9b",
   "metadata": {},
   "source": [
    "# Device and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba40be4-4af7-424f-869e-5e35f9387a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce84bbe6-d988-4df1-ae66-710731236c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int, deterministic: bool = True) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e64dc-a5e0-43e0-80b3-b1011f9c9ef4",
   "metadata": {},
   "source": [
    "# Config and Run Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "697b6aff-b122-47ea-8e32-6e8f3b8ec99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(block_size=8, batch_size=32, train_val_split=0.9, level='word', d_model=128, num_heads=4, num_layers=2, d_ff=256, learning_rate=0.0003, max_steps=3000, eval_interval=200, seed=123)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    block_size: int = 8\n",
    "    batch_size: int = 32\n",
    "    train_val_split: float = 0.9\n",
    "    level: str = \"word\"            # \"char\" or \"word\"\n",
    "\n",
    "    # Model\n",
    "    d_model: int = 128\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 2\n",
    "    d_ff: int = 256\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate: float = 3e-4\n",
    "    max_steps: int = 3000\n",
    "    eval_interval: int = 200\n",
    "\n",
    "    # Reproducibility\n",
    "    seed: int = 123\n",
    "\n",
    "@dataclass\n",
    "class RunRecord:\n",
    "    config: dict\n",
    "    created_at: str\n",
    "    run_dir: str\n",
    "    final_step: int = 0\n",
    "    final_train_loss: float = float(\"nan\")\n",
    "    final_val_loss: float = float(\"nan\")\n",
    "\n",
    "cfg = Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785fd074-6674-495e-b29d-92101f586fb3",
   "metadata": {},
   "source": [
    "# Run Directory and JSON helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "835f56b9-f1a7-4eb0-a5d5-db5ba7597d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_RUN_DIR = Path(\"runs\")\n",
    "\n",
    "def make_run_dir(base_dir: Path, cfg: Config) -> Path:\n",
    "    stamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    name = f\"{stamp}_seed{cfg.seed}_tiny_transformer\"\n",
    "    run_dir = base_dir / name\n",
    "    (run_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "def save_json(path: Path, data: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915462c7-f08b-4812-b7d3-6e78de756168",
   "metadata": {},
   "source": [
    "# Set seed and create run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ccc93bd-536e-4179-b198-92348b7d3e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory: runs/20251207T113926_seed123_tiny_transformer\n"
     ]
    }
   ],
   "source": [
    "set_seed(cfg.seed, deterministic=True)\n",
    "run_dir = make_run_dir(BASE_RUN_DIR, cfg)\n",
    "print(\"Run directory:\", run_dir)\n",
    "\n",
    "# Initial run record\n",
    "record = RunRecord(\n",
    "    config=asdict(cfg),\n",
    "    created_at=datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    run_dir=str(run_dir),\n",
    ")\n",
    "\n",
    "save_json(run_dir / \"config.json\", asdict(cfg))\n",
    "save_json(run_dir / \"run_record.json\", asdict(record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8625265a-cac3-43c8-8cef-e9676225462f",
   "metadata": {},
   "source": [
    "# Add text pf Gekko speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff1f8c2b-fb86-41d4-b571-de8ae924fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (characters): 2439\n",
      "[Gekko:] Well, I appreciate the opportunity you're giving me, Mr. Cromwell, as the single largest \n",
      "shareholder in Teldar Paper, to speak. Well, ladies and gentlemen, we're not here to indulge in fanta\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\"[Gekko:] Well, I appreciate the opportunity you're giving me, Mr. Cromwell, as the single largest \n",
    "shareholder in Teldar Paper, to speak. Well, ladies and gentlemen, we're not here to indulge in fantasy, but \n",
    "in political and economic reality. America, America has become a second-rate power. Its trade deficit and \n",
    "its fiscal deficit are at nightmare proportions. Now, in the days of the free market, when our country was a \n",
    "top industrial power, there was accountability to the stockholder. The Carnegies, the Mellons, the men that \n",
    "built this great industrial empire, made sure of it because it was their money at stake. Today, management \n",
    "has no stake in the company! All together, these men sitting up here own less than 3 percent of the company. \n",
    "And where does Mr. Cromwell put his million-dollar salary? Not in Teldar stock; he owns less than 1 percent. \n",
    "You own the company. That's right -- you, the stockholder. And you are all being royally screwed over by these, \n",
    "these bureaucrats, with their steak lunches, their hunting and fishing trips, their corporate jets and golden \n",
    "parachutes. [Cromwell:] This is an outrage! You're out of line, Gekko! [Gekko:] Teldar Paper, Mr. Cromwell, Teldar \n",
    "Paper has 33 different vice presidents, each earning over 200 thousand dollars a year. Now, I have spent the last \n",
    "two months analyzing what all these guys do, and I still can't figure it out. One thing I do know is that our paper \n",
    "company lost 110 million dollars last year, and I'll bet that half of that was spent in all the paperwork going back \n",
    "and forth between all these vice presidents. The new law of evolution in corporate America seems to be survival of \n",
    "the unfittest. Well, in my book you either do it right or you get eliminated. In the last seven deals that I've been \n",
    "involved with, there were 2.5 million stockholders who have made a pretax profit of 12 billion dollars. Thank you. I am \n",
    "not a destroyer of companies. I am a liberator of them!The point is, ladies and gentleman, that greed -- for lack of a better \n",
    "word -- Greed is good. Greed is right. Greed works. Greed clarifies, cuts through, and captures the essence of the evolutionary \n",
    "spirit. Greed, in all of its forms -- greed for life, for money, for love, knowledge -- has marked the upward surge of mankind. \n",
    "And greed -- you mark my words -- will not only save Teldar Paper, but that other malfunctioning corporation called the USA. \n",
    "Thank you very much.\"\"\"\n",
    "\n",
    "print(\"Corpus length (characters):\", len(raw_text))\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a9c80-22dc-4fec-bc80-5d617033655e",
   "metadata": {},
   "source": [
    "# Vocabulary and Enconde / Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fa535cb-ddf3-4e7c-9a1c-87f720860b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization level: word\n",
      "Vocab size: 251\n",
      "test:    Greed is good.\n",
      "encoded: [22, 126, 111, 4]\n",
      "decoded: Greed is good.\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization level:\", cfg.level)\n",
    "\n",
    "if cfg.level == \"char\":\n",
    "    # ----- Character-level: each character is a token -----\n",
    "    tokens = list(raw_text)               # e.g. [\"T\", \"o\", \" \", \"b\", \"e\", ...]\n",
    "    vocab = sorted(set(tokens))\n",
    "    \n",
    "    stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "    itos = {i: ch for ch, i in stoi.items()}\n",
    "    \n",
    "    def encode(s: str):\n",
    "        \"\"\"Encode string to list of integer IDs (char-level).\"\"\"\n",
    "        return [stoi[ch] for ch in s]\n",
    "    \n",
    "    def decode(ids):\n",
    "        \"\"\"Decode list of integer IDs back to string (char-level).\"\"\"\n",
    "        return \"\".join(itos[i] for i in ids)\n",
    "\n",
    "elif cfg.level == \"word\":\n",
    "    # ----- Word-level: lightweight word + punctuation tokenizer -----\n",
    "    def tokenize(s: str):\n",
    "        \"\"\"\n",
    "        Split text into words and punctuation.\n",
    "        Example:\n",
    "          \"To be, or not to be.\" ->\n",
    "          [\"To\", \"be\", \",\", \"or\", \"not\", \"to\", \"be\", \".\"]\n",
    "        \"\"\"\n",
    "        # \\w+  = one or more word chars (letters/digits/_)\n",
    "        # \\S   = any non-whitespace character (captures punctuation)\n",
    "        return re.findall(r\"\\w+|\\S\", s)\n",
    "    \n",
    "    def detokenize(tokens):\n",
    "        \"\"\"\n",
    "        Join tokens back into a string, fixing spaces before punctuation.\n",
    "        \"\"\"\n",
    "        text = \" \".join(tokens)\n",
    "        # Remove space before common punctuation: \"word , next\" -> \"word, next\"\n",
    "        text = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", text)\n",
    "        # Optional: \"(\" + \" word\" -> \"(word\"\n",
    "        text = re.sub(r\"([\\(\\[\\{])\\s+\", r\"\\1\", text)\n",
    "        return text\n",
    "    \n",
    "    tokens = tokenize(raw_text)\n",
    "    vocab = sorted(set(tokens))\n",
    "    \n",
    "    stoi = {tok: i for i, tok in enumerate(vocab)}\n",
    "    itos = {i: tok for tok, i in stoi.items()}\n",
    "    \n",
    "    def encode(s: str):\n",
    "        \"\"\"Encode string to list of integer IDs (word-level).\"\"\"\n",
    "        return [stoi[t] for t in tokenize(s)]\n",
    "    \n",
    "    def decode(ids):\n",
    "        \"\"\"Decode list of integer IDs back to string (word-level).\"\"\"\n",
    "        toks = [itos[i] for i in ids]\n",
    "        return detokenize(toks)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown cfg.level: {cfg.level!r}\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Quick sanity test\n",
    "test = \"Greed is good.\"\n",
    "encoded = encode(test)\n",
    "decoded = decode(encoded)\n",
    "print(\"test:   \", test)\n",
    "print(\"encoded:\", encoded)\n",
    "print(\"decoded:\", decoded)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac746d3-4dda-41f6-8f9b-6b6f1c22e4be",
   "metadata": {},
   "source": [
    "# Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "578bd3ff-b0ae-4313-9a85-f66d1b15a367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Greed is good\n",
      "encoded: [22, 126, 111]\n",
      "decoded: Greed is good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Greed is good\"\n",
    "encoded = encode(test)\n",
    "decoded = decode(encoded)\n",
    "print(\"test:\", test)\n",
    "print(\"encoded:\", encoded)\n",
    "print(\"decoded:\", decoded)\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cef778-bde0-4ac3-b0f3-f2daaba870b6",
   "metadata": {},
   "source": [
    "# Create tensor and Train/Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "772022a9-2abc-4434-9d56-fd82a3f0ab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (number of tokens): torch.Size([532])\n",
      "Train tokens: 478 Val tokens: 54\n"
     ]
    }
   ],
   "source": [
    "# Turn full corpus into a long 1D tensor of token IDs\n",
    "data = torch.tensor(encode(raw_text), dtype=torch.long)\n",
    "print(\"Data shape (number of tokens):\", data.shape)\n",
    "\n",
    "# Train/val split\n",
    "n = int(len(data) * cfg.train_val_split)\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]\n",
    "print(\"Train tokens:\", len(train_data), \"Val tokens:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd24288-69ad-4b62-ad02-3a13895bc7c1",
   "metadata": {},
   "source": [
    "# Batch sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3266ba08-18be-4683-8cbf-203dfdd8ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str):\n",
    "    source = train_data if split == \"train\" else val_data\n",
    "    B, T = cfg.batch_size, cfg.block_size\n",
    "\n",
    "    if len(source) <= T + 1:\n",
    "        raise ValueError(\n",
    "            f\"Not enough tokens in {split} split \"\n",
    "            f\"for block_size={T}. len(source)={len(source)}.\"\n",
    "        )\n",
    "\n",
    "    max_start = len(source) - T - 1\n",
    "    ix = torch.randint(0, max_start, (B,))\n",
    "    x = torch.stack([source[i : i + T] for i in ix])\n",
    "    y = torch.stack([source[i + 1 : i + 1 + T] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe2258-e3e1-48d3-86ff-86c81535b99a",
   "metadata": {},
   "source": [
    "# Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "049e04f9-19b8-43eb-b1f5-2379fc688516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q: Tensor, k: Tensor, v: Tensor, mask: Tensor | None = None) -> Tensor:\n",
    "    \"\"\"\n",
    "    q, k, v: [B, H, T, Hd]\n",
    "    mask:   [T, T] or None (1 = keep, 0 = mask out)\n",
    "    Returns: [B, H, T, Hd]\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    scores = q @ k.transpose(-2, -1) / math.sqrt(d_k)  # [B, H, T, T]\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "    attn = torch.softmax(scores, dim=-1) # should be [B, H, T, T]\n",
    "    out = attn @ v                       # should be [B, H, T, Hd]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c53bbf-1329-40a4-92f5-c5154ada3e1c",
   "metadata": {},
   "source": [
    "# Multi Head Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8300aa-3687-4d1f-9672-415cac803f54",
   "metadata": {},
   "source": [
    "This class asks \"what tokens should I pay attention to\". Multi head self attention enables each token in the sequence to look at other tokens, decide which ines arerelevant, and gather information from them.\n",
    "\n",
    "for language modelling this is how the model:\n",
    "\n",
    "    - understands dependencies\n",
    "    - tracks long range relationships\n",
    "    - carries semantic information forward\n",
    "    - builds contextual meaning\n",
    "\n",
    "<b><u>What happens inside this class:</b></u>\n",
    "\n",
    "At a high level for each position t:\n",
    "    1. the model creates a *query, key, value* for every token.\n",
    "    2. it computes *similarity scores* between the query at position t and the keys at all earlier positions.\n",
    "    3. Using softmax it converts these scores into *attention weights*.\n",
    "    4. It uses these weights to take a *weighted sum of the value vectors*.\n",
    "    5. This becomes the representation of the token in the next layer\n",
    "\n",
    "<b><u>Why \"multi-head\":</b></u>\n",
    "Each head can learn a different pattern eg it is possible that:\n",
    "    * One head tracks syntax\n",
    "    * One tracks names\n",
    "    * One tracks verb tense\n",
    "    * One tracks quotation boundaries, and so on...\n",
    "\n",
    "<b><u>Why Causal Masking:</b></u>\n",
    "It is important that token t does not see the future (i.e. t+1, t+2,...) otherwise themodel would cheat during training. Masking ensure that the model is truly able to learn * Given the past, predict the next token*\n",
    "\n",
    "### MultiHeadSelfAttention teaches the model how tokens relate to each other and forms the backbone of reasoning and contextual understanding in LLM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edca7a19-3c59-49aa-9716-f5c61ff8143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, block_size: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer(\"causal_mask\", mask)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, T, D = x.shape\n",
    "        H, Hd = self.num_heads, self.head_dim\n",
    "\n",
    "        q = self.W_q(x)  # [B, T, D]\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        # [B, T, D] -> [B,, H, T, Hd]\n",
    "        q = q.view(B, T, H, Hd).transpose(1, 2)\n",
    "        k = k.view(B, T, H, Hd).transpose(1, 2)\n",
    "        v = v.view(B, T, H, Hd).transpose(1, 2)\n",
    "\n",
    "        out = scaled_dot_product_attention(q, k, v, mask=self.causal_mask[:T, :T])\n",
    "\n",
    "        # [B, H, T, Hd] -> [B, T, D]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.W_o(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40a85b-3f67-4f4b-a7a7-70d50fdb43ff",
   "metadata": {},
   "source": [
    "# FeedForward "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20ccf8-ee97-4170-8c28-9c6d67188b16",
   "metadata": {},
   "source": [
    "### Purpose in an LLM - \"Transform the information that I've gathered\n",
    "\n",
    "After self attention mixes information across positions, the feedforward network applies a *nonlinear transformation* to each token independently. This step allows the model to:\n",
    "\n",
    "    - extract higher level features\n",
    "    - create richer innternal representations\n",
    "    - perform local computations over each tokesn embedding\n",
    "    - increase the models expressive capacity\n",
    "\n",
    "<b><u>What is does</b></u>\n",
    "Inside the TransformerBlock, FeedForward component is implemented using PyTorch's nn.Sequential, which applies a small neural network of the form:\n",
    "\n",
    "$\\text{FFN}(x) = W_2\\,\\mathrm{ReLU}(W_1 x)$\n",
    "\n",
    "This: \n",
    "\n",
    "    - Increases dimensionality (*expansion* via W1)\n",
    "    - Applies a nonlinerity (ReLU)\n",
    "    - Project back to the original dimension (*compression* via W2)\n",
    "\n",
    "This pattern (expand -> nonlinear -> compress) lets the transformer model complex functions that cannot be captured by attention alone.\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1afee0cd-d9ef-41d7-ad60-45e04fdf9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5e96a-b318-4381-b563-83b66aaac07f",
   "metadata": {},
   "source": [
    "# TransformerBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6663bc5-893d-4c00-8006-ff7e602a4296",
   "metadata": {},
   "source": [
    "A *TransformerBlock* combines several key components that work together to progressivley refine token representations as information flows through the model. Each block begins with <i>layer normalisation</i>, followed by <i>multi-head self attention</i>, which allows eaxh token to gather contextual information from earlier tokens in the sequence. A <i>residual connection</i> then preserves the original representation while adding the newly computed attention output, stabilising learning and improving gradient flow. This block applies a second layer normalisation before passing each token through a <i>feedforward network</i>, which performs a nonlinear transformation that enriches and expands the representation locally. Another residdual connection integrates this transformation with the tokens prior state. Stacking multiple blocks in depth enables the model to build a representational hierarchy: lower layers capture syrface-level patterns such as character sequences, middle layers learn grammatical structure and short range semantics, and upper layers develop broader more abstract meaning. Residual pathways throughout the block - mathematically in the form $x_{\\text{next}} = x + f(x)$ ensure stable optimisation by allowing gradients to propogate effectively, preventing representational collaose and making deeper transformers trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41912a26-b78a-4a17-a57a-e2d71c87f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, block_size: int):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads, block_size)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # GPT-style pre-norm + residual\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da2af6-f625-4601-91c5-af8f56016463",
   "metadata": {},
   "source": [
    "# TinyTransformerLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258872f-07af-4377-a7ae-e8fe1ae0c4fe",
   "metadata": {},
   "source": [
    "The TinyTransformer \"Token -> Meaning -> Prediction\" wraps the entire transformer architecture and produces logits at each position, which we use for next-token prediction.\n",
    "\n",
    "<b><u>Workflow inside the forward</u></b>\n",
    "\n",
    "1. Token embeddings - convert token id's into vectors\n",
    "2. Position embeddings - add information about position in the sequence\n",
    "3. Pass through L Transformer blocks - each block refines and contextualises the embeddings\n",
    "4. FInal `LayerNorm` and `Linear` Layer - convert the final hidden states into Logits over the vocabulary\n",
    "\n",
    "<b><u>Why Logits</u></b>\n",
    "\n",
    "Becuase `CrossEntropyLoss` takes Logits and computes - logP0(Xt | X < t). This creates a next token prediction becuase given input tokens X0, X1, X2,...,X(t - 1) the model predicts a districution over x(t) for every position t in the sequence simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "741b9f07-dae1-410e-8e15-7bb77f1784ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 329472\n"
     ]
    }
   ],
   "source": [
    "class TinyTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.token_emb = nn.Embedding(vocab_size, cfg.d_model)\n",
    "        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg.d_model, cfg.num_heads, cfg.d_ff, cfg.block_size)\n",
    "            for _ in range(cfg.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        self.head = nn.Linear(cfg.d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Optional: weight tying\n",
    "        # self.head.weight = self.token_emb.weight\n",
    "\n",
    "    def forward(self, idx: Tensor) -> Tensor:\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.cfg.block_size, \"Sequence length > block_size\"\n",
    "\n",
    "        tok = self.token_emb(idx)                         # [B, T, D]\n",
    "        pos_ids = torch.arange(T, device=idx.device)\n",
    "        pos = self.pos_emb(pos_ids)[None, :, :]           # [1, T, D]\n",
    "        x = tok + pos                                     # [B, T, D]\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                             # [B, T, V]\n",
    "        return logits\n",
    "\n",
    "model = TinyTransformerLM(vocab_size, cfg).to(device)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acebcf2-ceac-442c-bbcd-f9fdc6771b5b",
   "metadata": {},
   "source": [
    "# Loss Estimation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3d3c1f5-711e-4678-8c7c-e6730a3b696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):  # a few mini-batches\n",
    "            xb, yb = get_batch(split)      # <<--- use get_batch, not get_batch_split\n",
    "            logits = model(xb)\n",
    "            B, T, V = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B * T, V),\n",
    "                yb.view(B * T),\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03f51e-2882-42b2-bafe-603910550022",
   "metadata": {},
   "source": [
    "# Training Loop with logging & checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4056053e-41ee-4273-9ad5-1c8d2ea1dc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0: train loss 5.6555, val loss 5.7886\n",
      "  → New best val loss, checkpoint saved to runs/20251207T113926_seed123_tiny_transformer/checkpoints/best.pt\n",
      "step   200: train loss 1.3826, val loss 5.2300\n",
      "  → New best val loss, checkpoint saved to runs/20251207T113926_seed123_tiny_transformer/checkpoints/best.pt\n",
      "step   400: train loss 0.3611, val loss 5.8539\n",
      "step   600: train loss 0.2407, val loss 5.9890\n",
      "step   800: train loss 0.2000, val loss 6.2337\n",
      "step  1000: train loss 0.1777, val loss 6.4424\n",
      "step  1200: train loss 0.1874, val loss 6.5780\n",
      "step  1400: train loss 0.1748, val loss 6.7443\n",
      "step  1600: train loss 0.1733, val loss 6.9648\n",
      "step  1800: train loss 0.1896, val loss 7.0861\n",
      "step  2000: train loss 0.1766, val loss 7.0600\n",
      "step  2200: train loss 0.1802, val loss 7.2909\n",
      "step  2400: train loss 0.1615, val loss 7.1715\n",
      "step  2600: train loss 0.1646, val loss 7.2619\n",
      "step  2800: train loss 0.1837, val loss 7.4530\n",
      "Final checkpoint saved to: runs/20251207T113926_seed123_tiny_transformer/checkpoints/last.pt\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for step in range(cfg.max_steps):\n",
    "    # Periodic evaluation\n",
    "    if step % cfg.eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_loss, val_loss = losses[\"train\"], losses[\"val\"]\n",
    "        print(f\"step {step:5d}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "\n",
    "        # Update run record\n",
    "        record.final_step = step\n",
    "        record.final_train_loss = float(train_loss)\n",
    "        record.final_val_loss = float(val_loss)\n",
    "        save_json(run_dir / \"run_record.json\", asdict(record))\n",
    "\n",
    "        # Save checkpoint if best so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            ckpt_path = run_dir / \"checkpoints\" / \"best.pt\"\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"  → New best val loss, checkpoint saved to {ckpt_path}\")\n",
    "\n",
    "    # Sample a training batch\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Forward\n",
    "    logits = model(xb)                         # [B, T, V]\n",
    "    B, T, V = logits.shape\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(B * T, V),\n",
    "        yb.view(B * T),\n",
    "    )\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Optional gradient clipping\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "# Final save of model\n",
    "final_ckpt = run_dir / \"checkpoints\" / \"last.pt\"\n",
    "torch.save(model.state_dict(), final_ckpt)\n",
    "print(\"Final checkpoint saved to:\", final_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b24c5-1028-4936-a00d-2c9cd055b94d",
   "metadata": {},
   "source": [
    "# Sampling / Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2ca9b9d-9f0c-4e0c-b59c-02ae9e11c4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountability to the stockholder. The Carnegies, the Mellons, the men that built this great industrial empire, made sure of it because it was their money at stake. Today, management has no stake in the company! All together, these men sitting up here own\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model: TinyTransformerLM, idx: Tensor, max_new_tokens: int) -> Tensor:\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -cfg.block_size:]   # crop context\n",
    "        logits = model(idx_cond)              # [1, T, V]\n",
    "        logits = logits[:, -1, :]             # last position [1, V]\n",
    "        probs = torch.softmax(logits, dim=-1) # [1, V]\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    return idx\n",
    "\n",
    "# Example: Start from \"T\"\n",
    "start_gekko_letter=\"accountability\"\n",
    "generate_gekko_tokens=50\n",
    "start_ids = torch.tensor([[stoi[start_gekko_letter]]], dtype=torch.long, device=device)\n",
    "sample_ids = generate(model, start_ids, max_new_tokens=generate_gekko_tokens)\n",
    "print(decode(sample_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8ee72-dfcc-4f15-ba5e-fdf92f4dff74",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2315b9b-fff2-43f1-bc84-da9a2bf730c8",
   "metadata": {},
   "source": [
    "This toy experiment highlights how strongly a model’s behaviour depends on both the <b><u>tokenisation level and the amount of training data</b></u>. When using word-level tokenisation on an extremely small corpus, the model has very few unique tokens and very limited contextual variety. In this setting, the model cannot learn meaningful generalisations about language; instead, it effectively builds a shallow transition table over the handful of words it has seen. As a result, when prompted with a word such as “Greed” from the Gekko text, the model simply predicts one of the next words it encountered in the tiny training text. The apparent “generation” is therefore little more than a recombination or repetition of the original training sentences. This is expected and entirely appropriate for a dataset and model of this scale.\n",
    "\n",
    "In contrast, character-level tokenisation offers a much larger vocabulary and a finer-grained modelling space, even when the dataset itself is small. Although a tiny character-level model still cannot learn complex semantics, it can capture patterns in spelling, punctuation, and short substrings. This often allows it to generate novel character sequences—sometimes slightly garbled versions of the training text—that reflect an attempt to generalise beyond direct memorisation. In other words, character-level models tend to overfit differently: they memorise local structure rather than whole words, which gives them a little more flexibility.\n",
    "\n",
    "Together, these observations demonstrate a central principle in language modelling: <b><u>the capacity of the model and the richness of the token vocabulary must be matched to the amount of data available</b></u>. With tiny datasets, both word- and character-level models inevitably overfit, but in distinct ways. Word-level models tend to reproduce the training corpus almost verbatim, while character-level models exhibit more local creativity but remain fundamentally constrained. Understanding this helps build intuition for why modern LLMs rely on subword tokenisation and large-scale corpora—both choices that dramatically expand a model’s ability to generalise beyond memorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1223b-6493-4792-914a-8f1f1ca5ec5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (theAIE)",
   "language": "python",
   "name": "theaie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
