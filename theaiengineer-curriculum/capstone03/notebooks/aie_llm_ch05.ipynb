{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a14308a-92b0-4b00-895a-107110fb11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feba696c-bcd2-45f0-9a98-9ca944a86b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3]), device(type='cpu'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=torch.float32).reshape(2,3)\n",
    "y = torch.ones_like(x)\n",
    "x.shape, y.shape, x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7237cdb6-860a-4100-9d36-94ff933b9ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74d269d-1b58-4046-90d9-2a4aca163a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([10.0, 20.0, 30.0])\n",
    "(x+b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03f15a8-fc71-43e3-b96c-0278b0417054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 21., 32.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x+b)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c276fc6-f09b-4b29-93d5-57d7a0aa65fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=(\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf000c99-6951-4c45-9410-5b48c1bbc630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ed156-e1d5-49ca-bba9-246c83d4a8ae",
   "metadata": {},
   "source": [
    "***Autograd in a nutshell***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f92234-1d08-4e2b-af96-0f26f26a1e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=torch.tensor([2.0,-3.0,0.5],requires_grad=True)\n",
    "v=torch.tensor([1.0,2.0,3.0])\n",
    "loss=(w*v).sum()\n",
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3604ad7a-40e6-4ad6-8f5c-a05db663b4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cedd6fd5-685f-4501-b52a-62b964844c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.nn.Linear(3,1)\n",
    "opt=torch.optim.AdamW(model.parameters(),lr=3e-3)\n",
    "crit=torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c18303-af77-4c75-a792-153cc50d0db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787dc811-88e8-4b2c-b690-398431570837",
   "metadata": {},
   "source": [
    "***Linear Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f64877d-372a-4ecc-89e7-f97df23f1183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17.6037\n",
      "100 2.0197\n",
      "200 0.1039\n",
      "300 0.01\n",
      "400 0.0087\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device='cpu'\n",
    "w_true=torch.tensor([2.0,-3.5],device=device)\n",
    "b_true=torch.tensor(0.5, device=device)\n",
    "N=128\n",
    "X=torch.randn(N,2,device=device)\n",
    "y=(X@w_true)+b_true+0.1*torch.randn(N, device=device)\n",
    "model=torch.nn.Linear(2,1).to(device)\n",
    "opt=torch.optim.Adam(model.parameters(),lr=3e-2)\n",
    "loss_fn=torch.nn.MSELoss()\n",
    "\n",
    "for step in range(401):\n",
    "    opt.zero_grad()\n",
    "    pred=model(X).squeeze(-1)\n",
    "    loss=loss_fn(pred, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if step% 100 ==0:\n",
    "        print(step, round(loss.item(),4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92350a-0533-46ad-a473-3e31758f8385",
   "metadata": {},
   "source": [
    "***From ids to vectors***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "039550fc-a840-4c1c-be0b-488c4df2088c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ch6_tokenize import SimpleTokenizer\n",
    "tok = SimpleTokenizer.from_file('mini.txt',level='char')\n",
    "ids=tok.encode('Hello world')\n",
    "tok.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b30911-836a-4743-a969-8bad39cf6a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=torch.nn.Embedding(num_embeddings=len(tok.vocab),embedding_dim=8)\n",
    "batch=[tok.encode('Hello'),tok.encode('Vectors')]\n",
    "lens=max(len(x) for x in batch)\n",
    "# pad to the same length (simple left pad with PAD=0)\n",
    "P=tok.pad\n",
    "x=torch.tensor([s+[P]*(lens-len(s)) for s in batch])\n",
    "E(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4845cba8-ed66-48f8-9a14-2ec967d39c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  4,  2,  2,  3,  0,  0],\n",
       "        [ 1,  4, 14, 15,  3,  7, 16]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ccf33-77a0-4832-9d49-6b0de49d4424",
   "metadata": {},
   "source": [
    "***word level example***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df6f2872-6e05-4b7e-805a-b4925770ef89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ch6_tokenize import SimpleTokenizer\n",
    "tok_w = SimpleTokenizer.from_file('mini.txt', level='word')\n",
    "len(tok.vocab), len(tok_w.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03dfbafc-c896-4dc2-bba3-fa797fd94eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_w.encode('Hello vectors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f98c7-0d21-46b5-997c-ccd97a41a350",
   "metadata": {},
   "source": [
    "***Padding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8d89381-9618-48e0-a1de-bdf1a9ab3239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[5, 4, 2, 2, 3, 0, 0], [13, 4, 14, 15, 3, 7, 16]],\n",
       " [[0, 0, 5, 4, 2, 2, 3], [13, 4, 14, 15, 3, 7, 16]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = tok.pad\n",
    "batch = [tok.encode('Hello'), tok.encode('vectors')]\n",
    "L = max(len(s) for s in batch)\n",
    "right_pad = [s + [P] * (L- len(s)) for s in batch]\n",
    "left_pad = [[P] * (L-len(s)) + s for s in batch]\n",
    "right_pad, left_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb319b-9585-4c49-9710-89204179a98c",
   "metadata": {},
   "source": [
    "***Attention masks***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fb8ba7b-6f6b-4ba9-aec4-f0eb0efabdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 7]), torch.Size([7, 7]), torch.Size([2, 7, 7]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(right_pad)\n",
    "pad_mask = (x != P).float()\n",
    "T = x.size(1)\n",
    "causal = torch.tril(torch.ones(T,T))\n",
    "combined = pad_mask[:, None, :] * causal\n",
    "pad_mask.shape, causal.shape,combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100e447-5ea0-42af-9f19-e6bc0b1cd545",
   "metadata": {},
   "source": [
    "***Tiny NumPy Implementation of scaled Dot-Product Attention***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49b9b979-acd0-46af-b2ce-12d457ca48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def scaled_dot_product_attention(X, WQ, WK, WV, causal=True):\n",
    "    \"\"\"\n",
    "    X: (T, d_model) token embeddings\n",
    "    WQ: (d_model, d_K)\n",
    "    WK: (d_model, d_K)\n",
    "    WV: (d_model, d_v)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. build Q, K, V\n",
    "    Q = X @ WQ # (T, d_k)\n",
    "    K = X @ WK # (T, d_k)\n",
    "    V = X @ WV # (T, d_v)\n",
    "\n",
    "    # 2. Dot produects between all queries and keys\n",
    "    dk = Q.shape[1]\n",
    "    S = (Q @ K.T) / np.sqrt(dk)\n",
    "\n",
    "    # 3. causal mask: prevent attending to future tokens\n",
    "    if causal:\n",
    "        T = S.shape[0]\n",
    "        mask = np.triu(np.ones((T,T)), k=1) * -1e9 # -inf above diagonal\n",
    "        S = S + mask\n",
    "\n",
    "    # 4. Softmax along each row\n",
    "    S_exp = np.exp( S - np.max(S, axis=1,keepdims=True))\n",
    "    A = S_exp / np.sum(S_exp, axis = 1, keepdims =True)\n",
    "\n",
    "    # 5. Weighted sum of values\n",
    "    Y = A @ V # (T, d_v)\n",
    "\n",
    "    return Y, A # return outputs and attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1bfa44d-79af-49c8-bb37-81fb135851b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y:\n",
      " [[ 0.47457463 -4.50608804  4.82442894 -1.24297684  4.05417016]\n",
      " [ 0.47531953 -4.50529114  4.82360616 -1.24222098  4.05263508]\n",
      " [ 0.4980276  -4.28557389  4.65295421 -1.13288223  3.86366691]\n",
      " [-0.97233489  2.97425006 -2.13287955 -3.93428412 -1.86363758]]\n",
      "|attention weights A:\n",
      " [[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.99767274e-01 2.32725656e-04 0.00000000e+00 0.00000000e+00]\n",
      " [9.43771823e-01 7.13238239e-05 5.61568536e-02 0.00000000e+00]\n",
      " [1.72126286e-03 5.60257549e-02 1.32270120e-01 8.09982862e-01]]\n"
     ]
    }
   ],
   "source": [
    "T = 4       # sequence length\n",
    "d_model = 6 # input embedding size+(20/60)\n",
    "d_k = 4     # query / key size\n",
    "d_v = 5     # value size\n",
    "\n",
    "# Random toy data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(T, d_model)\n",
    "WQ = np.random.randn(d_model, d_k)\n",
    "WK = np.random.randn(d_model, d_k)\n",
    "WV = np.random.randn(d_model, d_v)\n",
    "\n",
    "Y, A = scaled_dot_product_attention(X, WQ, WK, WV)\n",
    "\n",
    "print(\"Output Y:\\n\", Y)\n",
    "print(\"|attention weights A:\\n\", A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e94465cc-f7b0-489a-8183-a66be0ac1764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.13940068, -1.23482582,  0.40234164, -0.68481009, -0.87079715],\n",
       "       [-0.57884966, -0.31155253,  0.05616534, -1.16514984,  0.90082649],\n",
       "       [ 0.46566244, -1.53624369,  1.48825219,  1.89588918,  1.17877957],\n",
       "       [-0.17992484, -1.07075262,  1.05445173, -0.40317695,  1.22244507],\n",
       "       [ 0.20827498,  0.97663904,  0.3563664 ,  0.70657317,  0.01050002],\n",
       "       [ 1.78587049,  0.12691209,  0.40198936,  1.8831507 , -1.34775906]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 3       # sequence length\n",
    "d_model = 6 # input embedding size+(20/60)\n",
    "d_k = 2     # query / key size\n",
    "d_v = 2    # value size\n",
    "\n",
    "# Random toy data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(T, d_model)\n",
    "WQ = np.random.randn(d_model, d_k)\n",
    "WK = np.random.randn(d_model, d_k)\n",
    "WV = np.random.randn(d_model, d_v)\n",
    "\n",
    "Y, A = scaled_dot_product_attention(X, WQ, WK, WV)\n",
    "\n",
    "print(\"Output Y:\\n\", Y)\n",
    "print(\"|attention weights A:\\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cf3a3-a88d-4fd2-81a3-c0b282e36c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (theAIE)",
   "language": "python",
   "name": "theaie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
