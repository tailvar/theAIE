{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b37b11-c2bb-49ec-8d63-9dbf0d89edb4",
   "metadata": {},
   "source": [
    "***===== Imports =====***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4501e80-5651-41cc-acb2-4d340cbaca03",
   "metadata": {},
   "source": [
    "*Pull in PyTorch and other required packages. Everything is kept deliberately minimal (just 'torch', 'nn', 'functional' and 'math' so that the mechanics of attention stay as transparent as possible*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29db25c3-8bec-486f-b63a-7c02f3386b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586669b-b66d-4816-bd3c-1e7700b9b02a",
   "metadata": {},
   "source": [
    "***===== dot product module =====***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deca0ba-4e43-4226-8f00-7cd854016833",
   "metadata": {},
   "source": [
    "*This cell implements teh fundamental attention operation used throughout the transformer.*\n",
    "\n",
    "*Given queries **Q**, keys **K** and values **V**, attention computes,*\n",
    "\n",
    "$$S=\\frac{Q K^\\top}{\\sqrt{d_k}},\n",
    "\\qquad\n",
    "A=\\mathrm{softmax}(s),\n",
    "\\qquad\n",
    "Y = A V.$$\n",
    "                   \n",
    "**S** is a matrix of all pairwise query-key similarity scores.\n",
    "**A** is the Attention matrix (row-wise softmax so each row sums to 1).\n",
    "**Y** is the final weighting sum of the value vectors.\n",
    "\n",
    "**The function supports batched inputs of shape '(..., T, d_k/d_v)' and an optional boolean or float mask that zeros out future positions (causal mask) or padded tokens. Returniing the attention matrix makes debugging and visualisation easier**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3129b7f2-cf40-4e36-b262-562691a0949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)\n",
    "\n",
    "    if mask is not None:\n",
    "        if mask.dtype == torch.bool:\n",
    "            scores = scores.masked_fill(~mask, -1e9)\n",
    "        else:\n",
    "            scores = scores + mask\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    Y = attn @ V\n",
    "    return Y, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dcfe3-4a22-4c80-a023-51e4d224b1ee",
   "metadata": {},
   "source": [
    "***===== Causal mask helper =====***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2becb-4b46-4a5a-8aa3-848042bbd292",
   "metadata": {},
   "source": [
    "A causal transformer should never be allowed position *t* to attend to future tokens *j > t*. To enforce this I build a lower-triangle boolean mask (torch.tril) of shape '(1, T, T)':\n",
    "\n",
    "$$\\text{mask}_{t,j} = \n",
    "\\begin{cases}\n",
    "1 & j \\le t \\\\\n",
    "0 & j > t.\n",
    "    \\end{cases}$$\n",
    "\n",
    "The mask is broadcast across the batch dimension. When passed into attention, masked positions receive a large negative score (~-1e9) before the sofmax, meaning their probability effectively becomes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068adbaa-b6fe-4c73-8371-9697d088b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_causal_mask(x):\n",
    "    B, T, _ = x.shape\n",
    "    mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "    return mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ec324-9928-44fc-88fe-aecdfc10bf49",
   "metadata": {},
   "source": [
    "***===== SelfAttention module (single head) =====***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891b914-a6d7-4327-8729-3973565437bb",
   "metadata": {},
   "source": [
    "This nodule bundles three linear projections:\n",
    "\n",
    "$( Q = X W_Q)$\n",
    "$( K = X W_K)$\n",
    "$( V = X W_V)$\n",
    "\n",
    "and then calls the scaled_dot_product_attention module from the previous cell.\n",
    "\n",
    "Input shape is '(B, T, d_model)' and the output has shape '(B, T, d_v)'. By default I set 'd_l = d_v = d_model', which makes it compatible with residual connections in full transformer blocks.\n",
    "\n",
    "If 'causal=true', the layer uses the causal mask so each position only atteends to the past. This is wexactly the \"self-attention\" primitive used by GPT-style transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff00f83-8753-49dd-8228-b8ec5de68945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k=None, d_v=None, causal=True, return_attn=False, use_linear=True):\n",
    "        \"\"\"\n",
    "        use_linear=True: use nn.Linear for Q/K/V projections\n",
    "        use_linear =False: use nn.Parameter weight matrices and manual matmul\n",
    "        \n",
    "        nn.Linear is a learnable matrix transformation that converts token \n",
    "        embeddings into Queries, Keys and Values - the three different \n",
    "        vector spaces the attention mechanism uses.\n",
    "        \n",
    "        nn.Parameter is a learned weight matrix; this exposes the underlying\n",
    "        matrix multiply explicitly, but is mathematically identical to using\n",
    "        nn.Linear without bias\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        d_k = d_k or d_model\n",
    "        d_v = d_v or d_model\n",
    "\n",
    "        self.causal = causal\n",
    "        self.return_attn = return_attn\n",
    "        self.use_linear=use_linear\n",
    "\n",
    "        # =============================\n",
    "        # Option 1 nn.Linear (clean, high level, recommendede)\n",
    "        # =============================\n",
    "        if self.use_linear:\n",
    "            # each linear layer holds a weight matrix AND handles the\n",
    "            # batch/sequence dimensions for us.\n",
    "            # No bias is needed for Q/K/V projections\n",
    "            self.W_Q = nn.Linear(d_model, d_k, bias=False)\n",
    "            self.W_K = nn.Linear(d_model, d_k, bias=False)\n",
    "            self.W_V = nn.Linear(d_model, d_k, bias=False)\n",
    "        else:\n",
    "            # ==========================\n",
    "            # ption 2: nn.Parameter + manual matrix multiplication\n",
    "            # ==========================\n",
    "            # we explicitly store weight matrices\n",
    "            # W_Q \\in \\mathbb{R}^(d_model x d_k}\n",
    "            # W_K \\in \\mathbb{R}^(d_model x d_k}\n",
    "            # W_V \\in \\mathbb{R}^(d_model x d_k}\n",
    "            #\n",
    "            # This is what linear does internally, except here we expose\n",
    "            # the raw matrices to do that matmul ourselves.\n",
    "            #\n",
    "            # The shapes use [d_model, d_k] so that;\n",
    "            # (B, T, d_model) @ (d_model, d_k) -> (B, T, d_k)\n",
    "            self.W_Q = nn.Parameter(torch.randn(d_model, d_k) * (1 / d_model))\n",
    "            self.W_K = nn.Parameter(torch.randn(d_model, d_k) * (1 / d_model))\n",
    "            self.W_V = nn.Parameter(torch.randn(d_model, d_k) * (1 / d_model))\n",
    "            \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, _ = x.shape\n",
    "        if self.use_linear:\n",
    "            # High level linear layers automatically apply:\n",
    "            # Q = x W_Q^T, K = x W_K^T, V = x W_V^T\n",
    "            Q = self.W_Q(x)\n",
    "            K = self.W_K(x)\n",
    "            V = self.W_V(x)\n",
    "        else:\n",
    "            # Manual weight matrices using nn.Parameter\n",
    "            # perform explicit matrix multiplies:\n",
    "            #\n",
    "            # Q[b, t] = x[b,t]@W_Q -> shape (d_k)\n",
    "            #\n",
    "            # same for K, V\n",
    "            #\n",
    "            Q = x @ self.W_Q # (B, T, d_k)\n",
    "            K = x @ self.W_K # (B, T, d_k)\n",
    "            V = x @ self.W_V # (B, T, d_k)\n",
    "\n",
    "        if mask is None and self.causal:\n",
    "            mask = make_causal_mask(x)\n",
    "\n",
    "        Y, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "        if self.return_attn:\n",
    "            return Y, attn\n",
    "        return Y        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c7bad-2715-45db-946e-6d56dd754456",
   "metadata": {},
   "source": [
    "***===== Sanity check =====***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59bcca-939e-481d-8377-b22f2ea25992",
   "metadata": {},
   "source": [
    "To confirm that the 'SelfAttention' module is correct, I directly compare:\n",
    "\n",
    "1. The output of the 'SelfAttention' module on an input 'x'.\n",
    "2. The output of manually computing $( Q = X W_Q)$, $( K = X W_K)$, $( V = X W_V)$, building the causal mask, and calling the 'scaled_dot_product_attention' module explicitlly.\n",
    "\n",
    "Both paths should agree up to a level of tolerance:\n",
    "\n",
    "$$y_{\\text{module}} \\approx y_{\\text{manual}}, \\qquad\n",
    "A_{\\text{module}} \\approx A_{\\text{manual}}.$$\n",
    "\n",
    "Running this test once gives me the confidence that the implementation bahaves exactly as intended and matches the mathematical definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce107d4-550f-48d8-be48-a2b73162db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention (nn.Parameter) matches 'scaled_dot_product_attention()' Yay\n"
     ]
    }
   ],
   "source": [
    "def test_self_attention(use_linear: bool):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    B, T, d_model = 2, 3, 4\n",
    "    x = torch.randn(B, T, d_model)\n",
    "\n",
    "    sa = SelfAttention(d_model, causal=True, return_attn=True, use_linear=False)\n",
    "    # run via module\n",
    "    y1, attn1 = sa(x)\n",
    "\n",
    "    # Run manually reconstruct Q, K, V using SAME weights ---\n",
    "    if isinstance(sa.W_Q, nn.Linear):\n",
    "        # nn.Linear version:\n",
    "        # 'a learnable matrix transformation that converts token embeddings into\n",
    "        # queries, keys and values - three different vector spaces the \n",
    "        # attention mechanism uses\n",
    "        Q = sa.W_Q(x)\n",
    "        K = sa.W_K(x)\n",
    "        V = sa.W_V(x)\n",
    "    else:\n",
    "        # nn>parameter version:\n",
    "        # a manually learned weight matrix, this exposes the underlying\n",
    "        # matrix multiply explicitly, but is mathematically identical to\n",
    "        # using nn.Linear without bias\n",
    "        Q = x @ sa.W_Q\n",
    "        K = x @ sa.W_K\n",
    "        V = x @ sa.W_V\n",
    "    mask = make_causal_mask(x)\n",
    "\n",
    "    y2, attn2 = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "    assert torch.allclose(y1, y2, atol=1e-6)\n",
    "    assert torch.allclose(attn1, attn2, atol=1e-6)\n",
    "\n",
    "    mode = \"nn.Linear\" if isinstance(sa.W_Q, nn.Linear) else \"nn.Parameter\"\n",
    "    print(f\"SelfAttention ({mode}) matches 'scaled_dot_product_attention()' Yay\")\n",
    "\n",
    "test_self_attention(use_linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e43981-1235-4629-aabd-470e6123da95",
   "metadata": {},
   "source": [
    "Understanding self-attention is much easier when i can see how each token attends to every other token. Visualising the attention matrix\n",
    "\n",
    "$$ A \\in \\mathbb{R}^{T \\timesT}\n",
    "\n",
    "produced by the attention layer helps this. Recalling that self attention produces weights by computing:\n",
    "\n",
    "$$S = \\frac{QK^\\top}{\\sqrt{d_k}}.$$\n",
    "\n",
    "and optionally adding a mask (causal or padding) t\n",
    "\n",
    "$$S' = S + \\text{mask}$$\n",
    "\n",
    "The attention weights are obtained by applying a row-wise softmax:\n",
    "\n",
    "$$A = \\operatorname{softmax}(S')\n",
    "\n",
    "Finally the output of attention is the weighted sum of values:\n",
    "\n",
    "$$Y = A V$$\n",
    "\n",
    "***What the Heatmap shows***\n",
    "* Each *row* of A corressponds to a *query position t*.\n",
    "* Each *column* corresponds to a *key/value position j*.\n",
    "* The intensity of a cell is the attention weight $\\alpha_{t j}$.\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffecf48-ed4f-47d5-833d-33cbcb0a41ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x=torch.randn(1,6,8)\n",
    "\n",
    "sa=SelfAttention(d_model=8, causal=True, return_attn=True)\n",
    "y, attn = sa(x)\n",
    "\n",
    "# remove batch dimension for plotting\n",
    "A = attn[0].detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(A, cmap='viridis')\n",
    "plt.title(\"Self-Attention Heatmap (causal mask)\")\n",
    "plt.xlabel(\"Key / Value position\")\n",
    "plt.ylabel(\"Query position\")\n",
    "plt.colorbar(label=\"Attention weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b557e48-1179-4b6d-aba1-fc51a9b04dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_self_attention_reduces_to_average():\n",
    "    \"\"\"\n",
    "    Verify that with W__Q =0, W_K = 0 and W_CV = I (and no causal mask),\n",
    "    self attention reduces to a simple average over positions.\n",
    "\n",
    "    For each batch item b abd time t:\n",
    "        y[b,t,:] should equal the mean over all positions j of x[b, j, :].\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    B, T, d_model = 2,4,3\n",
    "    x = torch.randn(B, T, d_model)\n",
    "\n",
    "    # use the nn.Parameter path so we can explicitly set W_Q, W_K and W_V\n",
    "    sa = SelfAttention(\n",
    "        d_mdel=d_model,\n",
    "        d_k=d_model,\n",
    "        d_v=d_model,\n",
    "        causal=False,\n",
    "        returns_attn=False,\n",
    "        use_linear=False)\n",
    "\n",
    "    # W_Q = 0, W_K = 0, W_V = I\n",
    "    with torch.no_grad():\n",
    "        sa.W_Q.zero_()\n",
    "        sa.W_K.zero_()\n",
    "        sa.W_V.copy.copy_(torch.eye(d_model))\n",
    "\n",
    "    # run through the layer\n",
    "    y, attn = sa(x)\n",
    "\n",
    "    # Expected: simple average over positions, same at every t\n",
    "    # shape (B, 1, d_model), then broadcast to (B, T, d_model)\n",
    "    mean_over_T = x.mean(dim=1, keepdim=True)\n",
    "    expected_y = mean_over_T.expand_as(y)\n",
    "\n",
    "    assert torch.allclose(y, expected_y, atol=1e-6)\n",
    "    print(\"SelfAttention reduces to simple average over positions\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaec873c-fb5c-43d3-8b5a-ca9d1a4ad0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Naive reference implementation of scaled dot-product attention\n",
    "    using explicit Python loops. Only for small tensors in tests.\n",
    "\n",
    "    Q, K, V: (B, T, d_k/d_v)\n",
    "    mask:    broadcastable to (B, T, T) or None\n",
    "\n",
    "    returns:\n",
    "        Y:    (B, T, d_v)\n",
    "        attn: (B, T, T)\n",
    "    \"\"\"\n",
    "    B, T, d_k = Q.shape\n",
    "    d_v = V.shape[-1]\n",
    "    device = Q.device\n",
    "\n",
    "    # Ensure we have a float mask with -inf for masked positions, or None\n",
    "    if mask is not None:\n",
    "        # Broadcast mask to (B, T, T)\n",
    "        mask = mask.to(device)\n",
    "        if mask.dtype == torch.bool:\n",
    "            # True = keep, False = mask → convert to 0 or -1e9\n",
    "            mask_full = torch.zeros(B, T, T, device=device)\n",
    "            mask_full = mask_full.masked_fill(~mask, -1e9)\n",
    "        else:\n",
    "            mask_full = mask.expand(B, T, T)\n",
    "    else:\n",
    "        mask_full = None\n",
    "\n",
    "    Y = torch.zeros(B, T, d_v, device=device)\n",
    "    attn = torch.zeros(B, T, T, device=device)\n",
    "\n",
    "    for b in range(B):\n",
    "        for t in range(T):\n",
    "            # 1. Compute scores for all j\n",
    "            scores = torch.empty(T, device=device)\n",
    "            for j in range(T):\n",
    "                scores[j] = (Q[b, t] @ K[b, j]) / (d_k ** 0.5)\n",
    "                if mask_full is not None:\n",
    "                    scores[j] += mask_full[b, t, j]\n",
    "\n",
    "            # 2. Softmax over j\n",
    "            max_score = scores.max()\n",
    "            exp_scores = torch.exp(scores - max_score)\n",
    "            probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "            attn[b, t] = probs\n",
    "\n",
    "            # 3. Weighted sum over V\n",
    "            y_t = torch.zeros(d_v, device=device)\n",
    "            for j in range(T):\n",
    "                y_t += probs[j] * V[b, j]\n",
    "            Y[b, t] = y_t\n",
    "\n",
    "    return Y, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a275f66b-20f1-4553-9c50-0110039787de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scaled_dot_product_attention_matches_naive():\n",
    "    \"\"\"\n",
    "    Compare the vectorised scaled_dot_product_attention against\n",
    "    the naive double-loop reference implementation on a tiny example.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    B, T, d_k, d_v = 2, 3, 4, 5\n",
    "    Q = torch.randn(B, T, d_k)\n",
    "    K = torch.randn(B, T, d_k)\n",
    "    V = torch.randn(B, T, d_v)\n",
    "\n",
    "    # Example mask: causal\n",
    "    x_dummy = torch.zeros(B, T, d_k)\n",
    "    mask = make_causal_mask(x_dummy)  # (1, T, T)\n",
    "\n",
    "    Y_fast, A_fast = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "    Y_slow, A_slow = naive_scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "    assert torch.allclose(Y_fast, Y_slow, atol=1e-6)\n",
    "    assert torch.allclose(A_fast, A_slow, atol=1e-6)\n",
    "    print(\"Vectorised attention matches naive double-loop implementation ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69d9b8-5a63-403b-9084-e7fec37da794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (theAIE)",
   "language": "python",
   "name": "theaie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
